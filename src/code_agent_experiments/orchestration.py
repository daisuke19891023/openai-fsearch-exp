"""Experiment orchestration utilities for running scenario batches."""
from __future__ import annotations

from dataclasses import dataclass
from datetime import UTC, datetime
import json
import logging
from typing import TYPE_CHECKING, Any, Callable, Iterable, Protocol, Sequence

from code_agent_experiments.domain.models import (
    Metrics,
    RetrievalRecord,
    RunConfig,
    Scenario,
)
from code_agent_experiments.evaluation.metrics import aggregate_metrics, compute_retrieval_metrics

if TYPE_CHECKING:
    from pathlib import Path

logger = logging.getLogger(__name__)


class ScenarioExecutor(Protocol):
    """Protocol describing the callable that executes a scenario."""

    def __call__(
        self,
        run_config: RunConfig,
        scenario: Scenario,
        replicate_index: int,
    ) -> RetrievalRecord:  # pragma: no cover - protocol definition
        """Execute ``scenario`` for ``run_config`` and return a retrieval record."""
        ...


@dataclass(slots=True)
class FailureRecord:
    """Represents a failure to execute a scenario."""

    run_id: str
    scenario_id: str
    replicate_index: int
    error: str
    timestamp: datetime


@dataclass(slots=True)
class ExperimentRunResult:
    """Summary artefacts generated by an orchestrated experiment run."""

    metrics: list[Metrics]
    comparison: dict[str, dict[str, Any]]
    per_run: dict[str, dict[str, Any]]
    output_dir: Path
    records_path: Path
    metrics_path: Path
    summary_path: Path
    failures_path: Path | None
    failures: list[FailureRecord]


class ExperimentStorage:
    """Utility responsible for persisting experiment artefacts."""

    def __init__(self, root: Path) -> None:
        """Initialise storage with the directory ``root``."""
        self.root = root
        self.records_path = self.root / "retrieval_records.jsonl"
        self.metrics_path = self.root / "metrics.jsonl"
        self.summary_path = self.root / "comparison.json"
        self.run_configs_path = self.root / "run_configs.json"
        self.scenarios_path = self.root / "scenarios.json"
        self.failures_path = self.root / "failures.jsonl"

    def prepare(self) -> None:
        """Ensure the output directory exists."""
        self.root.mkdir(parents=True, exist_ok=True)

    def record_run_configs(self, run_configs: Sequence[RunConfig]) -> None:
        """Persist the provided ``run_configs`` payload to disk."""
        payload = [config.model_dump(mode="json") for config in run_configs]
        self.run_configs_path.write_text(
            json.dumps(payload, indent=2),
            encoding="utf-8",
        )

    def record_scenarios(self, scenarios: Sequence[Scenario]) -> None:
        """Persist the provided ``scenarios`` payload to disk."""
        payload = [scenario.model_dump(mode="json") for scenario in scenarios]
        self.scenarios_path.write_text(
            json.dumps(payload, indent=2),
            encoding="utf-8",
        )

    def append_record(self, record: RetrievalRecord) -> None:
        """Append a single retrieval ``record`` to the records log."""
        with self.records_path.open("a", encoding="utf-8") as handle:
            handle.write(record.model_dump_json() + "\n")

    def append_metrics(self, metrics: Metrics) -> None:
        """Append a metrics entry to the metrics log."""
        with self.metrics_path.open("a", encoding="utf-8") as handle:
            handle.write(metrics.model_dump_json() + "\n")

    def append_failure(self, failure: FailureRecord) -> None:
        """Append a failure telemetry entry for later inspection."""
        with self.failures_path.open("a", encoding="utf-8") as handle:
            payload = {
                "run_id": failure.run_id,
                "scenario_id": failure.scenario_id,
                "replicate_index": failure.replicate_index,
                "error": failure.error,
                "timestamp": failure.timestamp.isoformat(),
            }
            handle.write(json.dumps(payload) + "\n")

    def write_summary(
        self,
        *,
        per_run: dict[str, dict[str, Any]],
        comparison: dict[str, dict[str, Any]],
    ) -> Path:
        """Write aggregated ``per_run`` and ``comparison`` results to disk."""
        payload = {"runs": per_run, "strategies": comparison}
        self.summary_path.write_text(
            json.dumps(payload, indent=2),
            encoding="utf-8",
        )
        return self.summary_path


class ExperimentOrchestrator:
    """Coordinate execution of scenarios across one or more run configurations."""

    def __init__(self, executor: ScenarioExecutor, storage: ExperimentStorage) -> None:
        """Initialise the orchestrator with an executor and storage."""
        self._executor = executor
        self._storage = storage

    def run(
        self,
        run_configs: Sequence[RunConfig],
        scenarios: Sequence[Scenario],
    ) -> ExperimentRunResult:
        """Execute ``scenarios`` for every ``run_config`` using the configured executor."""
        if not run_configs:
            message = "At least one run configuration is required"
            raise ValueError(message)
        if not scenarios:
            message = "At least one scenario is required"
            raise ValueError(message)

        self._storage.prepare()
        self._storage.record_run_configs(run_configs)
        self._storage.record_scenarios(scenarios)

        metrics: list[Metrics] = []
        failures: list[FailureRecord] = []

        for run_config in run_configs:
            replicate_total = max(1, int(run_config.replicate))
            for replicate_index in range(replicate_total):
                run_id = self._replicated_run_id(run_config.id, replicate_index)
                for scenario in scenarios:
                    record = self._execute_single(
                        run_config,
                        scenario,
                        replicate_index,
                        run_id,
                        failures,
                    )
                    metrics_entry = compute_retrieval_metrics(scenario, record)
                    metrics.append(metrics_entry)
                    self._storage.append_record(record)
                    self._storage.append_metrics(metrics_entry)

        per_run = self._aggregate(metrics, key=lambda item: item.run_id)
        comparison = self._aggregate(metrics, key=lambda item: item.strategy)
        summary_path = self._storage.write_summary(per_run=per_run, comparison=comparison)

        failures_path = self._storage.failures_path if failures else None
        return ExperimentRunResult(
            metrics=metrics,
            comparison=comparison,
            per_run=per_run,
            output_dir=self._storage.root,
            records_path=self._storage.records_path,
            metrics_path=self._storage.metrics_path,
            summary_path=summary_path,
            failures_path=failures_path,
            failures=failures,
        )

    def _execute_single(
        self,
        run_config: RunConfig,
        scenario: Scenario,
        replicate_index: int,
        run_id: str,
        failures: list[FailureRecord],
    ) -> RetrievalRecord:
        try:
            record = self._executor(run_config, scenario, replicate_index)
        except Exception as exc:  # pragma: no cover - exception path handled via failure record
            logger.exception(
                "Scenario execution failed",  # pragma: no cover - logging side effect
                extra={
                    "run_id": run_config.id,
                    "scenario_id": scenario.id,
                    "replicate_index": replicate_index,
                },
            )
            record = self._failure_record(run_config, scenario, run_id)
            failure = FailureRecord(
                run_id=run_id,
                scenario_id=scenario.id,
                replicate_index=replicate_index,
                error=str(exc),
                timestamp=datetime.now(tz=UTC),
            )
            failures.append(failure)
            self._storage.append_failure(failure)
        else:
            record = self._normalize_record(record, run_config, scenario, run_id)
        return record

    def _normalize_record(
        self,
        record: RetrievalRecord,
        run_config: RunConfig,
        scenario: Scenario,
        run_id: str,
    ) -> RetrievalRecord:
        strategy = record.strategy or run_config.id
        return record.model_copy(
            update={
                "scenario_id": scenario.id,
                "run_id": run_id,
                "strategy": strategy,
            },
        )

    def _failure_record(
        self,
        run_config: RunConfig,
        scenario: Scenario,
        run_id: str,
    ) -> RetrievalRecord:
        return RetrievalRecord(
            scenario_id=scenario.id,
            run_id=run_id,
            strategy=run_config.id,
            candidates=[],
            tool_calls=[],
            elapsed_ms=0,
        )

    @staticmethod
    def _replicated_run_id(run_id: str, replicate_index: int) -> str:
        suffix = f"rep{replicate_index + 1:02d}"
        return f"{run_id}-{suffix}"

    @staticmethod
    def _aggregate(
        metrics: Iterable[Metrics],
        *,
        key: Callable[[Metrics], str],
    ) -> dict[str, dict[str, Any]]:
        groups: dict[str, list[Metrics]] = {}
        for metric in metrics:
            group_key = key(metric)
            groups.setdefault(group_key, []).append(metric)

        aggregated: dict[str, dict[str, Any]] = {}
        for group_key, items in groups.items():
            aggregated[group_key] = aggregate_metrics(items)
        return aggregated


def default_executor(
    run_config: RunConfig,
    scenario: Scenario,
    replicate_index: int,
) -> RetrievalRecord:
    """Fallback executor returning an empty retrieval record."""
    return RetrievalRecord(
        scenario_id=scenario.id,
        run_id=f"{run_config.id}-rep{replicate_index + 1:02d}",
        strategy=run_config.id,
        candidates=[],
        tool_calls=[],
        elapsed_ms=0,
    )


__all__ = [
    "ExperimentOrchestrator",
    "ExperimentRunResult",
    "ExperimentStorage",
    "FailureRecord",
    "ScenarioExecutor",
    "default_executor",
]
